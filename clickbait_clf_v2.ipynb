{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickbait Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame, Series, read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database lenght : 59172 \n",
      "Clickbait ratio: 0.5118637193267086\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>titles</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6574</td>\n",
       "      <td>6575</td>\n",
       "      <td>25 Things We Learned From Julia Louis-Dreyfus ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>39655</td>\n",
       "      <td>39656</td>\n",
       "      <td>John Brennan: Trump's 'Nazi Germany' tweet to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>44513</td>\n",
       "      <td>44514</td>\n",
       "      <td>TruthRevolt.org: ISIS Stands For \"Israeli Secr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>44205</td>\n",
       "      <td>44206</td>\n",
       "      <td>Peak Millennial? Cities Cant Assume a Continue...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11106</td>\n",
       "      <td>11107</td>\n",
       "      <td>This Entire City Is Made Out Of Ice And It Wil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     id                                             titles  clickbait\n",
       "0   6574   6575  25 Things We Learned From Julia Louis-Dreyfus ...          1\n",
       "1  39655  39656  John Brennan: Trump's 'Nazi Germany' tweet to ...          0\n",
       "2  44513  44514  TruthRevolt.org: ISIS Stands For \"Israeli Secr...          0\n",
       "3  44205  44206  Peak Millennial? Cities Cant Assume a Continue...          0\n",
       "4  11106  11107  This Entire City Is Made Out Of Ice And It Wil...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = read_csv(\"clickBait_Data.csv\")\n",
    "titles_len = len(titles)\n",
    "clckbt_ratio = len(titles[titles[\"clickbait\"]==0])/titles_len\n",
    "print(\"Database lenght : {} \\nClickbait ratio: {}\".format(titles_len, clckbt_ratio))\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing and Analysis\n",
    "\n",
    "### 2.1 Train test split\n",
    "\n",
    "Split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(titles['titles'], titles[\"clickbait\"],\n",
    "                                                    test_size=.1, random_state=42)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split again the train set into Train and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47928\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.1, random_state=42)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Words counting\n",
    "\n",
    "We implement the following parsing operations:\n",
    "\n",
    "- converts words to lower-case,\n",
    "- expand contractions,\n",
    "- remove punctuation,\n",
    "- lemmatize words\n",
    "\n",
    "It returns the list of all the words of the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import contractions\n",
    "import string\n",
    "\n",
    "def remove_contractions(string):\n",
    "    str_words = [contractions.fix(word) for word in string.lower().split()]\n",
    "    return ' '.join(str_words)\n",
    "\n",
    "def lemmatise_sentence(sentence):\n",
    "    sentence = remove_contractions(sentence.lower())\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation+'’‘'))\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lemmatised_sentence = []\n",
    "    for word, tag in pos_tag(word_tokenize(sentence.lower())):\n",
    "        if tag.startswith('NN'):\n",
    "            word_1 = word \n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            word_1 = word\n",
    "            pos = 'v'\n",
    "        elif tag.startswith('CD'):\n",
    "            word_1 = 'NUM'\n",
    "            pos = 'a'\n",
    "        else:\n",
    "            word_1 = word\n",
    "            pos = 'a'\n",
    "        lemmatised_sentence.append(lemmatiser.lemmatize(word_1, pos))\n",
    "    return lemmatised_sentence\n",
    "    \n",
    "\n",
    "def lemmatise_verbs(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation+'’‘'))\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lemmatised_verbs = []\n",
    "    for word, tag in pos_tag(word_tokenize(sentence.lower())):\n",
    "        if tag.startswith('VB'):\n",
    "            word_1 = word\n",
    "            pos = 'v'\n",
    "            lemmatised_verbs.append(lemmatiser.lemmatize(word_1, pos))\n",
    "    return lemmatised_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Make a vocabulary with the word frequencies__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def make_dict(titles_set, collect_verbs=False, rm_stopwords=False, rm_words=False, to_del=None): \n",
    "    dictionary = Counter()\n",
    "    \n",
    "    tot_str = ' '\n",
    "    for title in titles_set:\n",
    "        tot_str+=title+' '\n",
    "    \n",
    "    if collect_verbs:\n",
    "        dictionary = Counter(lemmatise_verbs(tot_str))\n",
    "    else:\n",
    "        dictionary = Counter(lemmatise_sentence(tot_str)) \n",
    "    \n",
    "    # Remove english stopwords\n",
    "    if rm_stopwords:\n",
    "        stop_words = stopwords.words('english')\n",
    "        for word in stop_words:\n",
    "            del dictionary[word]\n",
    "            \n",
    "    # Remove some words\n",
    "    if rm_words:\n",
    "        for word in to_del:\n",
    "            del dictionary[word]\n",
    "     \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate __5 different vocabnularies__: \n",
    "\n",
    "- common words in all titles\n",
    "- common words in clckbt titles\n",
    "- common words in non-clckbait titles\n",
    "- common verbs in clckbt titles\n",
    "- common versb in non-clckbt titles \n",
    "\n",
    "The following function generates the vocabulary via the `make_dict` function, it saves it in a `.txt` file.\n",
    "Finally, it loads the list of the `n_words` most common words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')    # shut up bs4 URL warning\n",
    "\n",
    "\n",
    "def most_common_words(n_words, txt_file, collect_verbs=False, rm_stopwords=False, rm_words=False, \n",
    "                      to_del=None, words_set=None):\n",
    "    \n",
    "    # try to load the dictionary from the .txt file.\n",
    "    try:\n",
    "        words_count = []\n",
    "        file = open(txt_file, 'r')\n",
    "        \n",
    "        for line in range(n_words):\n",
    "            words_count.append(file.readline().rstrip())\n",
    "        \n",
    "        file.close()\n",
    "        words_count = np.array(words_count)\n",
    "        print('Dictionary loaded.')\n",
    "    \n",
    "    # make_dict if the .txt doesn't exists\n",
    "    except FileNotFoundError:\n",
    "        print(\"There's no dictionary. Creating a new one.\")\n",
    "        words_count = make_dict(words_set, collect_verbs, rm_stopwords=rm_stopwords, rm_words=rm_words, to_del=to_del)\n",
    "        words_count = words_count.most_common(n_words)\n",
    "        file = open(txt_file, 'w+')\n",
    "        \n",
    "        for key, freq in words_count:\n",
    "            file.write(str(key)+'\\n')\n",
    "        \n",
    "        file.close()\n",
    "        words_count = most_common_words(n_words, txt_file, collect_verbs, rm_stopwords, rm_words, to_del, words_set)\n",
    "    \n",
    "    return words_count\n",
    "\n",
    "to_del = ['trump','donald','christmas','obama','president','america','harry','russian','russia','china',\n",
    "          'american']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded.\n",
      "Dictionary loaded.\n",
      "Dictionary loaded.\n",
      "Dictionary loaded.\n",
      "Dictionary loaded.\n"
     ]
    }
   ],
   "source": [
    "total_words_count = most_common_words(n_words=1000, txt_file='vocabularies/lem_total_words.txt',\n",
    "                                      rm_stopwords=True, words_set=X_train)\n",
    "\n",
    "clckbt_words_count = most_common_words(n_words=1000, txt_file='vocabularies/lem_clckbt_words.txt', \n",
    "                                       rm_stopwords=False, rm_words=True, to_del=to_del, \n",
    "                                       words_set=X_train[y_train==1])\n",
    "\n",
    "no_clckbt_words_count = most_common_words(n_words=1000, txt_file='vocabularies/lem_no_clckbt_words.txt',\n",
    "                                          rm_stopwords=False, words_set=X_train[y_train==0])\n",
    "\n",
    "clckbt_verbs_count = most_common_words(n_words=1000, txt_file='vocabularies/lem_clckbt_verbs.txt', \n",
    "                                       collect_verbs=True, rm_stopwords=False, words_set=X_train[y_train==1])\n",
    "\n",
    "\n",
    "no_clckbt_verbs_count = most_common_words(n_words=1000, txt_file='vocabularies/lem_no_clckbt_verbs.txt', \n",
    "                                          collect_verbs=True, rm_stopwords=False, words_set=X_train[y_train==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the 20 most common clckbt and no_clckbt words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>No Clickbait</td>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>NUM</td>\n",
       "      <td>in</td>\n",
       "      <td>of</td>\n",
       "      <td>be</td>\n",
       "      <td>for</td>\n",
       "      <td>and</td>\n",
       "      <td>on</td>\n",
       "      <td>trump</td>\n",
       "      <td>with</td>\n",
       "      <td>as</td>\n",
       "      <td>at</td>\n",
       "      <td>say</td>\n",
       "      <td>not</td>\n",
       "      <td>have</td>\n",
       "      <td>new</td>\n",
       "      <td>from</td>\n",
       "      <td>after</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Clickbait</td>\n",
       "      <td>NUM</td>\n",
       "      <td>be</td>\n",
       "      <td>the</td>\n",
       "      <td>to</td>\n",
       "      <td>you</td>\n",
       "      <td>of</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "      <td>and</td>\n",
       "      <td>for</td>\n",
       "      <td>will</td>\n",
       "      <td>that</td>\n",
       "      <td>not</td>\n",
       "      <td>have</td>\n",
       "      <td>on</td>\n",
       "      <td>do</td>\n",
       "      <td>what</td>\n",
       "      <td>your</td>\n",
       "      <td>with</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mixed</td>\n",
       "      <td>NUM</td>\n",
       "      <td>trump</td>\n",
       "      <td>say</td>\n",
       "      <td>new</td>\n",
       "      <td>make</td>\n",
       "      <td>people</td>\n",
       "      <td>get</td>\n",
       "      <td>year</td>\n",
       "      <td>woman</td>\n",
       "      <td>us</td>\n",
       "      <td>take</td>\n",
       "      <td>time</td>\n",
       "      <td>world</td>\n",
       "      <td>donald</td>\n",
       "      <td>look</td>\n",
       "      <td>see</td>\n",
       "      <td>thing</td>\n",
       "      <td>go</td>\n",
       "      <td>find</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>No Clckbt vbs</td>\n",
       "      <td>be</td>\n",
       "      <td>say</td>\n",
       "      <td>have</td>\n",
       "      <td>get</td>\n",
       "      <td>take</td>\n",
       "      <td>make</td>\n",
       "      <td>kill</td>\n",
       "      <td>do</td>\n",
       "      <td>find</td>\n",
       "      <td>go</td>\n",
       "      <td>trump</td>\n",
       "      <td>give</td>\n",
       "      <td>want</td>\n",
       "      <td>call</td>\n",
       "      <td>leave</td>\n",
       "      <td>help</td>\n",
       "      <td>hold</td>\n",
       "      <td>show</td>\n",
       "      <td>happen</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Clckbt vbs</td>\n",
       "      <td>be</td>\n",
       "      <td>have</td>\n",
       "      <td>make</td>\n",
       "      <td>do</td>\n",
       "      <td>get</td>\n",
       "      <td>say</td>\n",
       "      <td>see</td>\n",
       "      <td>know</td>\n",
       "      <td>look</td>\n",
       "      <td>take</td>\n",
       "      <td>go</td>\n",
       "      <td>happen</td>\n",
       "      <td>want</td>\n",
       "      <td>find</td>\n",
       "      <td>need</td>\n",
       "      <td>believe</td>\n",
       "      <td>think</td>\n",
       "      <td>heres</td>\n",
       "      <td>use</td>\n",
       "      <td>wont</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0      1     2    3     4       5     6     7      8      9  \\\n",
       "No Clickbait    to    the   NUM   in    of      be   for   and     on  trump   \n",
       "Clickbait      NUM     be   the   to   you      of    in  this    and    for   \n",
       "Mixed          NUM  trump   say  new  make  people   get  year  woman     us   \n",
       "No Clckbt vbs   be    say  have  get  take    make  kill    do   find     go   \n",
       "Clckbt vbs      be   have  make   do   get     say   see  know   look   take   \n",
       "\n",
       "                  10      11     12      13     14       15     16     17  \\\n",
       "No Clickbait    with      as     at     say    not     have    new   from   \n",
       "Clickbait       will    that    not    have     on       do   what   your   \n",
       "Mixed           take    time  world  donald   look      see  thing     go   \n",
       "No Clckbt vbs  trump    give   want    call  leave     help   hold   show   \n",
       "Clckbt vbs        go  happen   want    find   need  believe  think  heres   \n",
       "\n",
       "                   18    19  \n",
       "No Clickbait    after    by  \n",
       "Clickbait        with    it  \n",
       "Mixed            find   man  \n",
       "No Clckbt vbs  happen   win  \n",
       "Clckbt vbs        use  wont  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words = DataFrame({'No Clickbait': no_clckbt_words_count, 'Clickbait': clckbt_words_count, \n",
    "                          'Mixed': total_words_count, 'No Clckbt vbs': no_clckbt_verbs_count,\n",
    "                          'Clckbt vbs': clckbt_verbs_count})\n",
    "common_words[:20].transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating the Features\n",
    "\n",
    "Now convert features into vectors.\n",
    "Define new classe, `DigitalizeTitle`. \n",
    "\n",
    "Features: \n",
    "- 200 most common clckbt words\n",
    "- title length (in words)\n",
    "- stopwords ratio\n",
    "- contractions ratio\n",
    "- title starts with cardinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from contractions import contractions_dict\n",
    "\n",
    "def contractions_count(string):\n",
    "    words = string.split()\n",
    "    counter = 0\n",
    "    for word in words:\n",
    "        if word in list(contractions_dict.keys()):\n",
    "            counter += 1\n",
    "    return counter/len(words)\n",
    "\n",
    "# Generate the feature vectors\n",
    "def vectorize(title, dictionary, n_words, options=[True]*4 ):\n",
    "    freq_vec = np.zeros(n_words+sum(options))\n",
    "    title_words = make_dict([title], rm_stopwords=False)\n",
    "    \n",
    "    # options consist in num_first, n_contractions, stopword_ratio, #_tot_words\n",
    "    options_arr = np.zeros(len(options))\n",
    "    stop_words = np.array(stopwords.words('english'))\n",
    "\n",
    "    options_arr[0] = 1 if 'NUM' == list(title_words.keys())[0] else 0\n",
    "    options_arr[1] = contractions_count(title)\n",
    "    \n",
    "    for index, key in list(enumerate(dictionary[:n_words])):\n",
    "        freq_vec[index] = title_words[key]\n",
    "    \n",
    "    for key in title_words.keys():\n",
    "        options_arr[3] += title_words[key]\n",
    "        if key in stop_words:\n",
    "            options_arr[2] += title_words[key]\n",
    "        \n",
    "    options_arr[2] = options_arr[2]/options_arr[3]\n",
    "    \n",
    "    if sum(options)>1:\n",
    "        freq_vec[n_words:] = options_arr[options]\n",
    "    \n",
    "    return freq_vec\n",
    "\n",
    "\n",
    "\n",
    "# CLASS DigitalizeTitle\n",
    "class DigitalizeTitle(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, clckbt_dict, options=[True]*4): \n",
    "        self.clckbt_dict = clckbt_dict\n",
    "        self.options = options\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        clckbt_words_vec = np.zeros((len(X), 200+sum(self.options)))\n",
    "        \n",
    "        for index, title in enumerate(X):\n",
    "            clckbt_words_vec[index] = vectorize(title, self.clckbt_dict, 200, self.options)\n",
    "            #clckbt_words_vec[index, 200:] = count_words(title)\n",
    "            \n",
    "        return clckbt_words_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared = DigitalizeTitle(clckbt_dict=clckbt_words_count, options=[True]*4).fit_transform(X_train)\n",
    "X_train_mini, y_train_mini = X_train_prepared[:1000], y_train[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train some classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer\n",
    "\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train Naive Bayes on only features words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.688\n",
      "precision: 0.735\n",
      "recall: 0.592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from utilitiesuseful_functions import print_scores\n",
    "\n",
    "mnb_clf = MultinomialNB()\n",
    "\n",
    "mnb_clf_cv = cross_validate(mnb_clf, X_train_mini[:,:200], y_train_mini, cv=5, scoring=scorers, n_jobs=5)\n",
    "print_scores(mnb_clf_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.721\n",
      "precision: 0.766\n",
      "recall: 0.617\n"
     ]
    }
   ],
   "source": [
    "mnb_clf_cv = cross_validate(mnb_clf, X_train_prepared[:,:200], y_train, cv=5, scoring=scorers, n_jobs=5)\n",
    "print_scores(mnb_clf_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_clf.fit(X_train_prepared[:,:200], y_train)\n",
    "probabilities = mnb_clf.predict_proba(X_train_prepared[:,:200])[:, 1]\n",
    "probabilities = probabilities.reshape((len(probabilities), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train Random Forest on Naive Bayes probabilities and non-word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_forest = np.concatenate([probabilities, X_train_prepared[:, 200:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.684\n",
      "precision: 0.697\n",
      "recall: 0.650\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utilities import print_scores\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "forest_cv = cross_validate(forest_clf, X_train_forest[:1000], y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(forest_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   15.0s\n",
      "/anaconda3/envs/ml_py37/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   25.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.728\n",
      "Best params: {'max_depth': 2.4005654856361716, 'n_estimators': 188}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   28.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "param_distrib = {\"n_estimators\": list(range(1,500)), \"max_depth\": reciprocal(2,100)}\n",
    "\n",
    "rnd_srch_forest = RandomizedSearchCV(forest_clf, param_distributions=param_distrib,\n",
    "                                     cv=5, scoring='accuracy', random_state=42,\n",
    "                                     n_iter=100, verbose=5, n_jobs=-1)\n",
    "\n",
    "rnd_srch_forest.fit(X_train_forest[:1000], y_train_mini)\n",
    "\n",
    "print('Best score: %.3f' % rnd_srch_forest.best_score_)\n",
    "print('Best params:', rnd_srch_forest.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.732\n",
      "precision: 0.805\n",
      "recall: 0.594\n"
     ]
    }
   ],
   "source": [
    "forest_clf = rnd_srch_forest.best_estimator_\n",
    "\n",
    "forest_cv = cross_validate(forest_clf, X_train_forest, y_train, cv=5, scoring=scorers, n_jobs=5)\n",
    "print_scores(forest_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Train SVM on Naive Bayes probabilities and non-word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.731\n",
      "precision: 0.809\n",
      "recall: 0.606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svc_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "svc_cv = cross_validate(svc_clf, X_train_forest[:1000], y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(svc_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1576 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 3088 tasks      | elapsed:   14.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.736\n",
      "Best params: {'svm__C': 11.987095327524536, 'svm__gamma': 0.00701925543856288, 'svm__kernel': 'rbf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 5000 out of 5000 | elapsed:   23.7s finished\n"
     ]
    }
   ],
   "source": [
    "param_distrib = {'svm__kernel': ['rbf', 'poly'],\n",
    "                 'svm__C': uniform(1,20),\n",
    "                 'svm__gamma': reciprocal(.0001, .1),\n",
    "                }\n",
    "                 \n",
    "\n",
    "rnd_srch_svc = RandomizedSearchCV(svc_clf, param_distributions=param_distrib,\n",
    "                                  cv=5, scoring='accuracy', n_iter=1000, verbose=5, n_jobs=-1)\n",
    "\n",
    "rnd_srch_svc.fit(X_train_forest[:1000], y_train_mini)\n",
    "\n",
    "print('Best score: %.3f' % rnd_srch_svc.best_score_)\n",
    "print('Best params:', rnd_srch_svc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.733\n",
      "precision: 0.829\n",
      "recall: 0.570\n"
     ]
    }
   ],
   "source": [
    "svc_clf = rnd_srch_svc.best_estimator_\n",
    "\n",
    "svc_clf_cv = cross_validate(svc_clf, X_train_forest, y_train, cv=5, scoring=scorers, n_jobs=5)\n",
    "print_scores(svc_clf_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train naive Random Forest on all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.685\n",
      "precision: 0.714\n",
      "recall: 0.620\n"
     ]
    }
   ],
   "source": [
    "forest_clf_v1 = RandomForestClassifier(random_state=42)\n",
    "\n",
    "forest_cv_v1 = cross_validate(forest_clf_v1, X_train_mini, y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(forest_cv_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   32.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.723\n",
      "Best params: {'max_depth': 44.00626778554037, 'n_estimators': 379}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "param_distrib = {\"n_estimators\": list(range(1,500)), \"max_depth\": reciprocal(2,100)}\n",
    "\n",
    "rnd_srch_forest_v1 = RandomizedSearchCV(forest_clf_v1, param_distributions=param_distrib,\n",
    "                                     cv=5, scoring='accuracy', random_state=42,\n",
    "                                     n_iter=100, verbose=5, n_jobs=-1)\n",
    "\n",
    "rnd_srch_forest_v1.fit(X_train_mini, y_train_mini)\n",
    "\n",
    "print('Best score: %.3f' % rnd_srch_forest_v1.best_score_)\n",
    "print('Best params:', rnd_srch_forest_v1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.802\n",
      "precision: 0.875\n",
      "recall: 0.692\n"
     ]
    }
   ],
   "source": [
    "forest_clf_v1 = rnd_srch_forest_v1.best_estimator_\n",
    "\n",
    "forest_cv_v1 = cross_validate(forest_clf_v1, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=5)\n",
    "print_scores(forest_cv_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train naive SVM on all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.695\n",
      "precision: 0.702\n",
      "recall: 0.680\n"
     ]
    }
   ],
   "source": [
    "svc_clf_v1 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "svc_cv_v1 = cross_validate(svc_clf_v1, X_train_mini, y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\n",
    "print_scores(svc_cv_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   17.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.709\n",
      "Best params: {'svm__C': 3.6108754588885907, 'svm__gamma': 0.0005731628906640736, 'svm__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "param_distrib = {'svm__kernel': ['rbf', 'poly'],\n",
    "                 'svm__C': uniform(1,20),\n",
    "                 'svm__gamma': reciprocal(.0001, .1),\n",
    "                }\n",
    "\n",
    "rnd_srch_svc_v1 = RandomizedSearchCV(svc_clf_v1, param_distributions=param_distrib,\n",
    "                                  cv=5, scoring='accuracy', n_iter=100, verbose=5, n_jobs=-1)\n",
    "\n",
    "rnd_srch_svc_v1.fit(X_train_mini, y_train_mini)\n",
    "\n",
    "print('Best score: %.3f' % rnd_srch_svc_v1.best_score_)\n",
    "print('Best params:', rnd_srch_svc_v1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf_v1 = rnd_srch_svc_v1.best_estimator_\n",
    "\n",
    "svc_clf_cv_v1 = cross_validate(svc_clf_v1, X_train_prepared, y_train, cv=5, scoring=scorers, n_jobs=5)\n",
    "print_scores(svc_clf_cv_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Test on Validation set\n",
    "\n",
    "Evaluate the three classifiers we've built on the Validation set to see which performs better\n",
    "\n",
    "### Prepare titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_prepared = DigitalizeTitle(clckbt_dict=clckbt_words_count, options=[True]*4).transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on the test set: 0.723\n",
      "Precision: 0.759\n",
      "Recall: 0.625\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = mnb_clf.predict(X_val_prepared[:,:200])\n",
    "\n",
    "print('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\n",
    "print('Precision: %.3f' % precision_score(y_val, y_val_pred))\n",
    "print('Recall: %.3f' % recall_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_proba = mnb_clf.predict_proba(X_val_prepared[:,:200])[:,1]\n",
    "y_val_proba = y_val_proba.reshape((len(y_val), 1))\n",
    "X_val_forest = np.concatenate([y_val_proba, X_val_prepared[:, 200:]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on the test set: 0.726\n",
      "Precision: 0.791\n",
      "Recall: 0.588\n"
     ]
    }
   ],
   "source": [
    "forest_clf.fit(X_train_forest, y_train)\n",
    "y_val_pred = forest_clf.predict(X_val_forest)\n",
    "\n",
    "print('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\n",
    "print('Precision: %.3f' % precision_score(y_val, y_val_pred))\n",
    "print('Recall: %.3f' % recall_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on the test set: 0.731\n",
      "Precision: 0.817\n",
      "Recall: 0.570\n"
     ]
    }
   ],
   "source": [
    "svc_clf.fit(X_train_forest, y_train)\n",
    "y_val_pred = svc_clf.predict(X_val_forest)\n",
    "\n",
    "print('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\n",
    "print('Precision: %.3f' % precision_score(y_val, y_val_pred))\n",
    "print('Recall: %.3f' % recall_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on the test set: 0.817\n",
      "Precision: 0.895\n",
      "Recall: 0.705\n"
     ]
    }
   ],
   "source": [
    "forest_clf_v1.fit(X_train_prepared, y_train)\n",
    "y_val_pred = forest_clf_v1.predict(X_val_prepared)\n",
    "\n",
    "print('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\n",
    "print('Precision: %.3f' % precision_score(y_val, y_val_pred))\n",
    "print('Recall: %.3f' % recall_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Final evaluation on the Test Set\n",
    "\n",
    "Choose the first classifier that performs better than the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on the test set: 0.820\n",
      "Precision: 0.897\n",
      "Recall: 0.719\n"
     ]
    }
   ],
   "source": [
    "X_test_prep = DigitalizeTitle(clckbt_dict=clckbt_words_count, options=[True]*4).transform(X_test)\n",
    "y_test_pred = forest_clf_v1.predict(X_test_prep)\n",
    "\n",
    "print('Score on the test set: %.3f' % accuracy_score(y_test, y_test_pred))\n",
    "print('Precision: %.3f' % precision_score(y_test, y_test_pred))\n",
    "print('Recall: %.3f' % recall_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 7. Export the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clickbait_classifier_v2.joblib']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "filename = 'clickbait_classifier_v2.joblib'\n",
    "dump(forest_clf_v1, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
